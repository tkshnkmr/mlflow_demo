{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879a2a4e-456c-4894-8f91-5afad4e96892",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A simple Vertex AI pipeline demo\n",
    "- Apply the same transform to train and test data\n",
    "- Train model with hyperparam tuning\n",
    "- Evaluate model with test data\n",
    "- Deploy the model on Endpoint\n",
    "- Request inference results to the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57882869-f55d-4a03-94d4-e6445bae624f",
   "metadata": {},
   "source": [
    "### 0. Save the data on bucket\n",
    "- Download data on your local machine from Kaggle's website https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "- Upload the data on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9a77f-21b3-4401-b26f-77115de7e5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.cloud.storage as storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad2d39-e19a-417d-973a-cabb3e8a723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: it's not the best practice\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"\"  # Your service credentials (assume json)\n",
    "PROJECT_ID = \"\"  # Your project id\n",
    "SERVICE_ACCOUNT = \"\"  # assume XXXX@YYYYY.iam.gserviceaccount.com\n",
    "CSV_FILE_BUCKET_NAME = \"\"  # bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6373ea-67a4-4d32-ae9e-04b5d3dd7562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca1d2a-ff0e-4721-b957-0ca8fdb27c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiates a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.bucket(CSV_FILE_BUCKET_NAME)\n",
    "\n",
    "# Creates the new bucket\n",
    "# bucket = storage_client.create_bucket(CSV_FILE_BUCKET_NAME)\n",
    "\n",
    "FILE_DIR = \"house-prices-advanced-regression-techniques\"\n",
    "csv_files_in_dir = [x for x in os.listdir(FILE_DIR) if x.split(\".\")[-1] == \"csv\"]\n",
    "\n",
    "for file in csv_files_in_dir:\n",
    "    blob = bucket.blob(blob_name=file)\n",
    "    blob.upload_from_filename(filename=f\"{FILE_DIR}/{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779b8e1-3c8d-48bb-8adb-485128e9e441",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03890e-9dd7-4c2f-99f2-c608dfbb06f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import OutputPath\n",
    "from kfp.v2.dsl import InputPath\n",
    "from kfp.v2.dsl import Model\n",
    "from kfp.v2.dsl import Input\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Output\n",
    "from kfp.v2.dsl import Metrics\n",
    "from kfp.v2.dsl import Dataset\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1edb5a-373d-4c65-9137-e6c817942d60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters for GCP and kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fb8db-ebf1-4c67-90f4-81395ae68aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cache pipeline results to avoid compute the same components for different runs\n",
    "ENABLE_CACHING = True\n",
    "\n",
    "PIPELINE_NAME = \"my-kfp-on-gcp-demo-notebook\"\n",
    "# Your Kubeflow's detail\n",
    "TEMPLATE_PATH = \"ml_pipeline_from_notebook.json\"\n",
    "# GCS Bucket to store artefacts\n",
    "PIPELINE_ROOT = f\"gs://kfp-demo-bucket-{PROJECT_ID}\"\n",
    "\n",
    "# Run parameters\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOBID = f\"training-pipeline-{TIMESTAMP}\"\n",
    "\n",
    "# Params for ML\n",
    "MODEL_FEATURE_LS = [\n",
    "    \"OverallQual\",\n",
    "    \"GrLivArea\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"FullBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"YearBuilt\",\n",
    "    \"BsmtUnfSF_TotalBsmtSF_ratio\",\n",
    "]\n",
    "MODEL_LABEL = \"SalePrice\"\n",
    "MODEL_HYPER_PARAM = {\"alpha\": [0.9, 0.95, 1], \"l1_ratio\": [0.45, 0.5, 0.55]}\n",
    "\n",
    "# Model serving param\n",
    "BASE_CONTAINER_IMG = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "ENDPOINT_MODEL_NAME = \"my_kf_elasticnet_model_from_notebook\"\n",
    "\n",
    "# NB: these parameters must be added for your pipeline's args\n",
    "PIPELINE_PARAMS = {\n",
    "    \"project_id\": PROJECT_ID,\n",
    "    \"original_bucket_id\": CSV_FILE_BUCKET_NAME,\n",
    "    \"target_train_filename\": \"train.csv\",\n",
    "    \"target_test_filename\": \"test.csv\",\n",
    "    \"datasplit_seed\": 10,\n",
    "    \"selected_features\": MODEL_FEATURE_LS,\n",
    "    \"selected_label\": MODEL_LABEL,\n",
    "    \"my_hyper_params_dict\": MODEL_HYPER_PARAM,\n",
    "    \"serving_container_image_uri\": BASE_CONTAINER_IMG,\n",
    "    \"endpoint_machine_type\": ENDPOINT_MACHINE_TYPE,\n",
    "    \"endpoint_model_name\": ENDPOINT_MODEL_NAME,\n",
    "}\n",
    "\n",
    "# Package list and version\n",
    "GCP_AI_PLATFORM = \"google-cloud-aiplatform==1.18.1\"\n",
    "GCP_BUCKET = \"google-cloud-storage==1.43.0\"\n",
    "PANDAS = \"pandas==1.5.3\"\n",
    "PYTHON_BASE = \"python:3.10\"\n",
    "SKLEARN = \"scikit-learn==1.2.2\"\n",
    "NUMPY = \"numpy==1.23.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4357bc-c154-4c03-837e-88b94a441492",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6bbb4-83d0-4b78-9b6b-e94ba773bba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=PYTHON_BASE, packages_to_install=[PANDAS, GCP_BUCKET])\n",
    "def preprocess_my_data(\n",
    "    project_id: str,\n",
    "    original_bucket_id: str,\n",
    "    target_filename: str,\n",
    "    output_dataset: Output[Dataset],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Some preprocessing\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    import google.cloud.storage as storage\n",
    "\n",
    "    # ===============================\n",
    "    #  Collect data from storage\n",
    "    # ===============================\n",
    "    # Instantiates a client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(original_bucket_id)\n",
    "\n",
    "    # Download csv file from GCS\n",
    "    blob = bucket.blob(target_filename)  # train.csv or test.csv\n",
    "    data = blob.download_as_string()\n",
    "    df = pd.read_csv(io.BytesIO(data))\n",
    "\n",
    "    # ===============================\n",
    "    #  Some feature engineering\n",
    "    #  1. Mean fill + creating ratio\n",
    "    # ===============================\n",
    "    # Get mean value for the target column\n",
    "    mean_target_col = df[\"TotalBsmtSF\"].mean()\n",
    "    # Replace 0 value to mean\n",
    "    df[\"TotalBsmtSF_fillmean\"] = df[\"TotalBsmtSF\"].replace(0, mean_target_col)\n",
    "    # Get mean value for the target column\n",
    "    mean_target_col = df[\"BsmtUnfSF\"].mean()\n",
    "    # Replace 0 value to mean\n",
    "    df[\"BsmtUnfSF_fillmean\"] = df[\"BsmtUnfSF\"].replace(0, mean_target_col)\n",
    "    df[\"BsmtUnfSF_TotalBsmtSF_ratio\"] = (\n",
    "        df[\"BsmtUnfSF_fillmean\"] / df[\"TotalBsmtSF_fillmean\"]\n",
    "    )\n",
    "\n",
    "    # Create an output\n",
    "    df.to_csv(output_dataset.path, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860c5c5-83ce-4855-aea1-c1fa8a2e45cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f9375-0eb6-4167-9e57-f448bfd8e319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=PYTHON_BASE,\n",
    "    packages_to_install=[\n",
    "        PANDAS,\n",
    "        SKLEARN,\n",
    "        NUMPY,\n",
    "    ],\n",
    ")\n",
    "def train_my_ml_model(\n",
    "    input_dataset: Input[Dataset],\n",
    "    datasplit_seed: int,\n",
    "    selected_features: list,\n",
    "    selected_label: str,\n",
    "    my_hyper_params_dict: dict,\n",
    "    eval_metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    ") -> NamedTuple(\"Outputs\", [(\"val_mse\", float), (\"val_mae\", float), ('best_hyperparam', dict)]):\n",
    "    \"\"\"\n",
    "    Some training\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import (\n",
    "        train_test_split,\n",
    "        RandomizedSearchCV,\n",
    "        StratifiedKFold,\n",
    "    )\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import pickle\n",
    "\n",
    "    # Read table from upstream compnents\n",
    "    df = pd.read_csv(input_dataset.path)\n",
    "\n",
    "    # Split Features and Labels\n",
    "    X = df[selected_features]\n",
    "    assert X.isna().sum().sum() == 0  # NB: not the best practice\n",
    "    y = df[selected_label]\n",
    "\n",
    "    # Split the data for train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=datasplit_seed\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    elastic_net_reg = ElasticNet(random_state=42)\n",
    "    # Hyper-param tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        elastic_net_reg,\n",
    "        param_distributions=my_hyper_params_dict,\n",
    "        n_iter=8,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=4,\n",
    "        cv=5,\n",
    "        verbose=3,\n",
    "        random_state=1001,\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "    # Get the best\n",
    "    elastic_net_reg_best = random_search.best_estimator_\n",
    "    best_hyperparam = random_search.best_params_\n",
    "\n",
    "    # Get the trained model performance with validation data\n",
    "    y_val_pred = elastic_net_reg_best.predict(X_val)\n",
    "    val_mse = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_mae = mean_squared_error(y_val, y_val_pred)\n",
    "    metrics_dict = {\"val_mse\": val_mse, \"val_mae\": val_mae, \"best_hyper_param\": best_hyperparam}\n",
    "\n",
    "    # dumping metrics_dict\n",
    "    with open(eval_metrics.path, \"w\") as f:\n",
    "        json.dump(metrics_dict, f)\n",
    "\n",
    "    # Save the model\n",
    "    # NB: Apr-2023, joblib file had problem to deploy on endpoint\n",
    "    # NB: Model path should end model.pkl or model.joblib (i.e., model.path)\n",
    "    # https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#pickle_1\n",
    "    # joblib.dump(elastic_net_reg_best, f\"{model.path}.joblib\")\n",
    "    with open(f\"{model.path}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(elastic_net_reg_best, f)\n",
    "\n",
    "    return (val_mse, val_mae, best_hyperparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b34413-d62c-4472-80d7-7d31d4d02685",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06c5fb-f0ef-4eae-b7cd-c3ec25a7e29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=PYTHON_BASE,\n",
    "    packages_to_install=[\n",
    "        PANDAS,\n",
    "        SKLEARN,\n",
    "        NUMPY,\n",
    "    ],\n",
    ")\n",
    "def eval_my_ml_model(\n",
    "    input_dataset: Input[Dataset],\n",
    "    selected_features: list,\n",
    "    model: Input[Model],\n",
    "    pred_output_csv: Output[Dataset],\n",
    "    model_deployment_flag: Output[Artifact],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Some validation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import joblib\n",
    "    import pickle\n",
    "\n",
    "    # load the test data\n",
    "    df = pd.read_csv(input_dataset.path)\n",
    "\n",
    "    # Split Features and Labels\n",
    "    X = df[selected_features]\n",
    "    X.dropna(inplace=True)  # NB: not the best practice\n",
    "    assert X.isna().sum().sum() == 0  # NB: not the best practice\n",
    "\n",
    "    # load model\n",
    "    # elastic_net_reg = pickle.load(f\"{model.path}.pkl\")\n",
    "    with open(f\"{model.path}.pkl\", \"rb\") as f:\n",
    "        elastic_net_reg = pickle.load(f)\n",
    "\n",
    "    # Prediction towards to test data\n",
    "    y_test_pred = elastic_net_reg.predict(X)\n",
    "\n",
    "    # Create an output\n",
    "    np.savetxt(f\"{pred_output_csv.path}\", y_test_pred.round(1), delimiter=\",\")\n",
    "\n",
    "    # dumping metrics_dict\n",
    "    score = 4678  # NB: RANDOM number for demo\n",
    "    metrics_dict = {\"model_eval_passed\": True, \"threshould_or_score\": score}\n",
    "    with open(model_deployment_flag.path, \"w\") as f:\n",
    "        json.dump(metrics_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a50999-c5ba-4b2d-b326-76f0de39e78c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Deploy the model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad437d-3257-434a-a174-c8eec297af74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=PYTHON_BASE,\n",
    "    packages_to_install=[PANDAS, NUMPY, GCP_AI_PLATFORM],\n",
    ")\n",
    "def deploy_my_ml_model(\n",
    "    project_id: str,\n",
    "    model: Input[Model],\n",
    "    serving_container_image_uri: str,\n",
    "    endpoint_machine_type: str,\n",
    "    endpoint_model_name: str,\n",
    "    model_deployment_flag: Input[Artifact],\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Some deploying models\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # Get the flag value from upstream components\n",
    "    with open(model_deployment_flag.path, \"r\") as f:\n",
    "        str_flag = f.readline()\n",
    "    model_deployment_flag_json = json.loads(str_flag)\n",
    "\n",
    "    if not model_deployment_flag_json[\"model_eval_passed\"]:\n",
    "        print(\"Model failed to pass the threshold\")\n",
    "        model_deployment_flag.uri = \"N/A\"\n",
    "        vertex_endpoint.uri = \"N/A\"\n",
    "        vertex_model.uri = \"N/A\"\n",
    "\n",
    "    else:\n",
    "        aiplatform.init(project=project_id)\n",
    "\n",
    "        # List of pre-build docker images:\n",
    "        # https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "        # https://aiinpractice.com/gcp-mlops-vertex-ai-pipeline-scikit-learn/\n",
    "        # NOTE: using the base image, and this must be model.joblib or model.pkl\n",
    "        deployed_model = aiplatform.Model.upload(\n",
    "            display_name=endpoint_model_name,\n",
    "            artifact_uri=os.path.dirname(model.uri),\n",
    "            serving_container_image_uri=serving_container_image_uri,\n",
    "        )\n",
    "        endpoint = deployed_model.deploy(machine_type=endpoint_machine_type)\n",
    "\n",
    "        # Save data to the output params\n",
    "        vertex_endpoint.uri = endpoint.resource_name\n",
    "        vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d947c8-d96f-4cb8-847c-32f0c60d2a23",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82c109-ab47-43c5-9515-15a375e1aba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a pipeline and create a task from a component:\n",
    "@pipeline(name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT)\n",
    "def tmp_pipe(\n",
    "    project_id: str,\n",
    "    original_bucket_id: str,\n",
    "    target_train_filename: str,\n",
    "    target_test_filename: str,\n",
    "    datasplit_seed: int,\n",
    "    selected_features: list,\n",
    "    selected_label: str,\n",
    "    my_hyper_params_dict: dict,\n",
    "    serving_container_image_uri: str,\n",
    "    endpoint_model_name: str,\n",
    "    endpoint_machine_type: str,\n",
    "):\n",
    "    train_preprocess = preprocess_my_data(\n",
    "        project_id=project_id,\n",
    "        original_bucket_id=original_bucket_id,\n",
    "        target_filename=target_train_filename,\n",
    "    ).set_display_name(\"Preprocess train data\")\n",
    "\n",
    "    test_preprocess = preprocess_my_data(\n",
    "        project_id=project_id,\n",
    "        original_bucket_id=original_bucket_id,\n",
    "        target_filename=target_test_filename,\n",
    "    ).set_display_name(\"Preprocess test data\")\n",
    "\n",
    "    train_model = train_my_ml_model(\n",
    "        input_dataset=train_preprocess.outputs[\"output_dataset\"],\n",
    "        datasplit_seed=datasplit_seed,\n",
    "        selected_features=selected_features,\n",
    "        selected_label=selected_label,\n",
    "        my_hyper_params_dict=my_hyper_params_dict,\n",
    "    ).set_display_name(\"Train model\")\n",
    "\n",
    "    evaluation_model = eval_my_ml_model(\n",
    "        input_dataset=test_preprocess.outputs[\"output_dataset\"],\n",
    "        selected_features=selected_features,\n",
    "        model=train_model.outputs[\"model\"],\n",
    "    ).set_display_name(\"Model evaluation\")\n",
    "\n",
    "    deploy_model = deploy_my_ml_model(\n",
    "        project_id=project_id,\n",
    "        model=train_model.outputs[\"model\"],\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        endpoint_model_name=endpoint_model_name,\n",
    "        endpoint_machine_type=endpoint_machine_type,\n",
    "        model_deployment_flag=evaluation_model.outputs[\"model_deployment_flag\"],\n",
    "    ).set_display_name(\"Deploy model to endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a31a32-5bbb-4cdd-b444-e4b44033bc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=tmp_pipe, package_path=TEMPLATE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b84875-d6bc-4c91-a226-3e2cf06ff7de",
   "metadata": {},
   "source": [
    "### 6. Deploy to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955e391-ca6b-4057-b924-3204980a58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892edf7-94cf-4a93-a3b5-55dfe445d1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOBID = f\"training-pipeline-{TIMESTAMP}\"\n",
    "\n",
    "# Pipeline job\n",
    "pipeline_ = aiplatform.pipeline_jobs.PipelineJob(\n",
    "    enable_caching=ENABLE_CACHING,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    job_id=JOBID,\n",
    "    parameter_values=PIPELINE_PARAMS,\n",
    ")\n",
    "# Submit\n",
    "pipeline_.submit(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d8554-cb54-4029-add2-a178b4ca909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca497631-1860-47dd-b926-8af352c84751",
   "metadata": {},
   "source": [
    "### 7. Prediction (request to the endpoint)\n",
    "- Wait 10-15 minutes until the endpoint is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a701a90-5855-4628-add6-5280b3648054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update the endpoint location\n",
    "vertex_ai_model_endpoint = \"projects/YOUR_PROEJCT_NUMBER/locations/us-central1/endpoints/ENDPOINT_NUMBER\"\n",
    "endpoint = aiplatform.Endpoint(vertex_ai_model_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b2c39-f369-4f78-8ec6-7179c3269e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the endpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get some values from the train\n",
    "df = pd.read_csv(\"house-prices-advanced-regression-techniques/train.csv\")\n",
    "\n",
    "df_mod = df[\n",
    "    [\n",
    "        \"OverallQual\",\n",
    "        \"GrLivArea\",\n",
    "        \"GarageCars\",\n",
    "        \"GarageArea\",\n",
    "        \"TotalBsmtSF\",\n",
    "        \"1stFlrSF\",\n",
    "        \"FullBath\",\n",
    "        \"TotRmsAbvGrd\",\n",
    "        \"YearBuilt\",\n",
    "        \"SalePrice\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_mod.loc[[0, 4, 100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74680073-058b-46dd-a87f-f8296e9160b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample input (NB: added 0.5 value at the end - feature engineered value)\n",
    "input_instance = [\n",
    "    [7, 1710, 2, 548, 856, 856, 2, 8, 2003, 0.5],\n",
    "    [8, 2198, 3, 836, 1145, 1145, 2, 9, 2000, 0.5],\n",
    "    [6, 1610, 2, 480, 1610, 1610, 2, 6, 1977, 0.5],\n",
    "]\n",
    "\n",
    "endpoint.predict(instances=input_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c015c6-2567-44ac-bed7-fee7e95f800a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
